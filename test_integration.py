#!/usr/bin/env python3
"""
HVDC í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ v2.6

ìµœì‹  ì‹¤ì „ ì˜ˆì œ ë° í™•ì¥ ìë™í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
from pathlib import Path

# ëª¨ë“ˆ ì„í¬íŠ¸
from mapping_utils import (
    mapping_manager, 
    normalize_vendor, 
    standardize_container_columns,
    add_storage_type_to_dataframe,
    normalize_vendor_enhanced,
    standardize_container_columns_enhanced,
    add_storage_type_to_dataframe_enhanced,
    get_numeric_fields_from_mapping,
    validate_dataframe_against_mapping
)

from excel_reporter import (
    generate_monthly_in_out_stock_report,
    generate_excel_comprehensive_report,
    generate_automated_summary_report,
    validate_report_data
)

from ontology_mapper import (
    dataframe_to_rdf,
    create_enhanced_rdf,
    generate_sparql_queries,
    validate_rdf_conversion,
    create_rdf_schema,
    quick_rdf_convert
)

def create_test_data():
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        'Case_No': [f'CASE{i:03d}' for i in range(1, 21)],
        'Date': [
            '2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05',
            '2024-02-01', '2024-02-02', '2024-02-03', '2024-02-04', '2024-02-05',
            '2024-03-01', '2024-03-02', '2024-03-03', '2024-03-04', '2024-03-05',
            '2024-04-01', '2024-04-02', '2024-04-03', '2024-04-04', '2024-04-05'
        ],
        'Location': [
            'DSV Indoor', 'DSV Outdoor', 'DSV MZP', 'AGI', 'DAS',
            'DSV Indoor', 'DSV Outdoor', 'MOSB', 'MIR', 'SHU',
            'DSV Indoor', 'DSV Al Markaz', 'DSV MZP', 'AGI', 'DAS',
            'DSV Indoor', 'DSV Outdoor', 'MOSB', 'MIR', 'SHU'
        ],
        'Qty': np.random.randint(10, 1000, 20),
        'Amount': np.random.uniform(1000, 50000, 20),
        'Handling Fee': np.random.uniform(50, 500, 20),
        'Vendor': np.random.choice(['SIMENSE', 'HITACHI', 'SAMSUNG', 'ZENER', 'ETC'], 20),
        'TxType_Refined': np.random.choice(['IN', 'TRANSFER_OUT', 'FINAL_OUT'], 20),
        'CBM': np.random.uniform(1, 100, 20),
        'Weight (kg)': np.random.uniform(10, 1000, 20),
        '20FT': np.random.randint(0, 5, 20),
        '40FT': np.random.randint(0, 3, 20),
        '20FR': np.random.randint(0, 2, 20),
        '40FR': np.random.randint(0, 2, 20)
    }
    
    df = pd.DataFrame(test_data)
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)}ê±´")
    return df

def test_mapping_utils(df):
    """mapping_utils í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”§ mapping_utils í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # 1. ë²¤ë” ì •ê·œí™” í…ŒìŠ¤íŠ¸
    print("  ğŸ“ ë²¤ë” ì •ê·œí™” í…ŒìŠ¤íŠ¸...")
    df['Vendor_Normalized'] = df['Vendor'].apply(normalize_vendor)
    vendor_counts = df['Vendor_Normalized'].value_counts()
    print(f"    âœ… ë²¤ë” ì •ê·œí™” ì™„ë£Œ: {dict(vendor_counts)}")
    
    # 2. ì»¨í…Œì´ë„ˆ ì»¬ëŸ¼ í‘œì¤€í™” í…ŒìŠ¤íŠ¸
    print("  ğŸ“¦ ì»¨í…Œì´ë„ˆ ì»¬ëŸ¼ í‘œì¤€í™” í…ŒìŠ¤íŠ¸...")
    df_std = standardize_container_columns(df.copy())
    container_cols = [col for col in df_std.columns if any(ct in col for ct in ['20FT', '40FT', '20FR', '40FR'])]
    print(f"    âœ… ì»¨í…Œì´ë„ˆ ì»¬ëŸ¼ í‘œì¤€í™” ì™„ë£Œ: {container_cols}")
    
    # 3. Storage Type ì¶”ê°€ í…ŒìŠ¤íŠ¸
    print("  ğŸ·ï¸ Storage Type ì¶”ê°€ í…ŒìŠ¤íŠ¸...")
    df_storage = add_storage_type_to_dataframe(df.copy())
    storage_counts = df_storage['Storage_Type'].value_counts()
    print(f"    âœ… Storage Type ë¶„ë¥˜ ì™„ë£Œ: {dict(storage_counts)}")
    
    # 4. í–¥ìƒëœ í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸
    print("  ğŸš€ í–¥ìƒëœ í•¨ìˆ˜ë“¤ í…ŒìŠ¤íŠ¸...")
    df_enhanced = normalize_vendor_enhanced(df['Vendor'].iloc[0])
    print(f"    âœ… í–¥ìƒëœ ë²¤ë” ì •ê·œí™”: {df_enhanced}")
    
    # 5. ë§¤í•‘ ê²€ì¦ í…ŒìŠ¤íŠ¸
    print("  âœ… ë§¤í•‘ ê²€ì¦ í…ŒìŠ¤íŠ¸...")
    validation = validate_dataframe_against_mapping(df)
    print(f"    âœ… ë§¤í•‘ ê²€ì¦ ê²°ê³¼: {validation}")
    
    return df_storage

def test_excel_reporter(df):
    """excel_reporter í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“Š excel_reporter í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # 1. ê¸°ë³¸ ì›”ë³„ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸
    print("  ğŸ“ˆ ê¸°ë³¸ ì›”ë³„ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸...")
    in_df, out_df, stock_df = generate_monthly_in_out_stock_report(df)
    print(f"    âœ… IN ë¦¬í¬íŠ¸: {len(in_df)}ê±´")
    print(f"    âœ… OUT ë¦¬í¬íŠ¸: {len(out_df)}ê±´")
    print(f"    âœ… ì¬ê³  ë¦¬í¬íŠ¸: {len(stock_df)}ê±´")
    
    # 2. í†µí•© ì—‘ì…€ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸
    print("  ğŸ“‹ í†µí•© ì—‘ì…€ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸...")
    excel_path = generate_excel_comprehensive_report(df, output_file="test_output/integrated_report.xlsx")
    print(f"    âœ… í†µí•© ë¦¬í¬íŠ¸ ìƒì„±: {excel_path}")
    
    # 3. ìë™í™”ëœ ìš”ì•½ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸
    print("  ğŸ¤– ìë™í™”ëœ ìš”ì•½ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸...")
    summary_path = generate_automated_summary_report(df, output_dir="test_output")
    print(f"    âœ… ìë™í™” ë¦¬í¬íŠ¸ ìƒì„±: {summary_path}")
    
    # 4. ë°ì´í„° ê²€ì¦ í…ŒìŠ¤íŠ¸
    print("  âœ… ë°ì´í„° ê²€ì¦ í…ŒìŠ¤íŠ¸...")
    validation = validate_report_data(df)
    print(f"    âœ… ë°ì´í„° ê²€ì¦ ê²°ê³¼: {validation}")
    
    return excel_path, summary_path

def test_ontology_mapper(df):
    """ontology_mapper í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”— ontology_mapper í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # 1. ê¸°ë³¸ RDF ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("  ğŸ”— ê¸°ë³¸ RDF ë³€í™˜ í…ŒìŠ¤íŠ¸...")
    rdf_path = dataframe_to_rdf(df, "test_output/basic_output.ttl")
    print(f"    âœ… ê¸°ë³¸ RDF ë³€í™˜: {rdf_path}")
    
    # 2. í–¥ìƒëœ RDF ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("  ğŸš€ í–¥ìƒëœ RDF ë³€í™˜ í…ŒìŠ¤íŠ¸...")
    enhanced_rdf_path = create_enhanced_rdf(df, "test_output/enhanced_output.ttl")
    print(f"    âœ… í–¥ìƒëœ RDF ë³€í™˜: {enhanced_rdf_path}")
    
    # 3. SPARQL ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸
    print("  ğŸ” SPARQL ì¿¼ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸...")
    sparql_path = generate_sparql_queries("test_output")
    print(f"    âœ… SPARQL ì¿¼ë¦¬ ìƒì„±: {sparql_path}")
    
    # 4. RDF ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸
    print("  ğŸ“‹ RDF ìŠ¤í‚¤ë§ˆ ìƒì„± í…ŒìŠ¤íŠ¸...")
    schema_path = create_rdf_schema("test_output/schema.ttl")
    print(f"    âœ… RDF ìŠ¤í‚¤ë§ˆ ìƒì„±: {schema_path}")
    
    # 5. ë¹ ë¥¸ ë³€í™˜ í…ŒìŠ¤íŠ¸
    print("  âš¡ ë¹ ë¥¸ ë³€í™˜ í…ŒìŠ¤íŠ¸...")
    rdf_path, sparql_path, schema_path = quick_rdf_convert(df, "test_output")
    print(f"    âœ… ë¹ ë¥¸ ë³€í™˜ ì™„ë£Œ: {rdf_path}")
    
    # 6. RDF ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸
    print("  âœ… RDF ë³€í™˜ ê²€ì¦ í…ŒìŠ¤íŠ¸...")
    validation = validate_rdf_conversion(df)
    print(f"    âœ… RDF ë³€í™˜ ê²€ì¦: {validation}")
    
    return rdf_path, sparql_path, schema_path

def test_mapping_rules_integration():
    """mapping_rules í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“‹ mapping_rules í†µí•© í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    # 1. mapping_rules ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("  ğŸ“„ mapping_rules ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    try:
        with open('mapping_rules_v2.6.json', 'r', encoding='utf-8') as f:
            rules = json.load(f)
        print(f"    âœ… mapping_rules ë¡œë“œ ì™„ë£Œ: v{rules.get('version', 'unknown')}")
        
        # 2. í•„ë“œ ë§¤í•‘ í™•ì¸
        field_map = rules.get('field_map', {})
        print(f"    âœ… í•„ë“œ ë§¤í•‘ ìˆ˜: {len(field_map)}ê°œ")
        
        # 3. ìë™í™” ê¸°ëŠ¥ í™•ì¸
        automation = rules.get('automation_features', {})
        print(f"    âœ… ìë™í™” ê¸°ëŠ¥: {list(automation.keys())}")
        
        # 4. ìˆ«ìí˜• í•„ë“œ í™•ì¸
        numeric_fields = get_numeric_fields_from_mapping()
        print(f"    âœ… ìˆ«ìí˜• í•„ë“œ: {numeric_fields}")
        
    except Exception as e:
        print(f"    âŒ mapping_rules ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return True

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ HVDC í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path("test_output").mkdir(exist_ok=True)
    
    try:
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        df = create_test_data()
        
        # 2. mapping_rules í†µí•© í…ŒìŠ¤íŠ¸
        test_mapping_rules_integration()
        
        # 3. mapping_utils í…ŒìŠ¤íŠ¸
        df_processed = test_mapping_utils(df)
        
        # 4. excel_reporter í…ŒìŠ¤íŠ¸
        excel_path, summary_path = test_excel_reporter(df_processed)
        
        # 5. ontology_mapper í…ŒìŠ¤íŠ¸
        rdf_path, sparql_path, schema_path = test_ontology_mapper(df_processed)
        
        # 6. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 60)
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print(f"  ğŸ“Š ì—‘ì…€ ë¦¬í¬íŠ¸: {excel_path}")
        print(f"  ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_path}")
        print(f"  ğŸ”— RDF íŒŒì¼: {rdf_path}")
        print(f"  ğŸ” SPARQL ì¿¼ë¦¬: {sparql_path}")
        print(f"  ğŸ“‹ RDF ìŠ¤í‚¤ë§ˆ: {schema_path}")
        
        print("\nâœ… ìµœì‹  ì‹¤ì „ ì˜ˆì œ ë° í™•ì¥ ìë™í™” ê¸°ëŠ¥ì´ ëª¨ë‘ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("\nâŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!") 