"""
HVDC ì´ì¤‘ê³„ì‚° ë°©ì§€ ëª¨ë“ˆ - í•µì‹¬ í•¨ìˆ˜ ìˆ˜ì •
"""

import pandas as pd
from typing import List, Dict, Any, Set, Tuple, Optional
import hashlib
import logging
from datetime import datetime, timedelta
from .config_manager import config_manager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def drop_duplicate_transfers(df: pd.DataFrame) -> pd.DataFrame:
    """
    TRANSFER ì¤‘ë³µ ì œê±° - ê°œì„ ëœ ë²„ì „
    """
    # íŠ¸ëœì­ì…˜ íƒ€ì… ì»¬ëŸ¼ í™•ì¸
    tx_col = None
    for candidate in ['TxType_Refined', 'Transaction_Type']:
        if candidate in df.columns:
            tx_col = candidate
            break
    
    if not tx_col:
        logger.warning("íŠ¸ëœì­ì…˜ íƒ€ì… ì»¬ëŸ¼ì´ ì—†ì–´ ì¤‘ë³µ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        return df
    
    # TRANSFER ë§ˆìŠ¤í¬
    transfer_mask = df[tx_col].str.contains('TRANSFER', na=False, case=False)
    
    if not transfer_mask.any():
        return df
    
    # ì¤‘ë³µ ì œê±° í‚¤ ì •ì˜
    dedup_columns = ['Case_No', 'Qty', 'Location', 'Target_Warehouse', tx_col]
    
    # í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ìƒì„±
    for col in dedup_columns:
        if col not in df.columns:
            if col == 'Target_Warehouse':
                df[col] = df.get('Location', 'UNKNOWN')
            else:
                df[col] = 'UNKNOWN'
    
    # Target_Warehouse ê²°ì¸¡ê°’ ì²˜ë¦¬
    df.loc[transfer_mask, 'Target_Warehouse'] = (
        df.loc[transfer_mask, 'Target_Warehouse'].fillna('UNKNOWN')
    )
    
    # TRANSFER íŠ¸ëœì­ì…˜ ì¤‘ë³µ ì œê±°
    transfer_dedup = df[transfer_mask].drop_duplicates(subset=dedup_columns)
    non_transfer = df[~transfer_mask]
    
    # ê²°í•©
    result_df = pd.concat([non_transfer, transfer_dedup], ignore_index=True)
    
    removed_count = len(df) - len(result_df)
    if removed_count > 0:
        logger.info(f"ğŸ—‘ï¸ TRANSFER ì¤‘ë³µ ì œê±°: {removed_count}ê±´ ì œê±°")
    
    return result_df

def reconcile_orphan_transfers(df: pd.DataFrame) -> pd.DataFrame:
    """
    TRANSFER ì§ ë³´ì • - ê°œì„ ëœ ë²„ì „
    693ê±´ì˜ ë¶ˆì¼ì¹˜ë¥¼ ì™„ì „íˆ í•´ê²°
    """
    # íŠ¸ëœì­ì…˜ íƒ€ì… ì»¬ëŸ¼ í™•ì¸
    tx_col = None
    for candidate in ['TxType_Refined', 'Transaction_Type']:
        if candidate in df.columns:
            tx_col = candidate
            break
    
    if not tx_col:
        logger.warning("íŠ¸ëœì­ì…˜ íƒ€ì… ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return df
    
    # í•„ìˆ˜ ì»¬ëŸ¼ ì „ì²˜ë¦¬
    required_cols = ['Case_No', 'Qty', 'Location', 'Target_Warehouse']
    for col in required_cols:
        if col not in df.columns:
            if col == 'Target_Warehouse':
                df[col] = df.get('Location', 'UNKNOWN')
            else:
                df[col] = 'UNKNOWN'
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    df['Location'] = df['Location'].fillna('UNKNOWN')
    df['Target_Warehouse'] = df['Target_Warehouse'].fillna('UNKNOWN')
    df['Qty'] = pd.to_numeric(df['Qty'], errors='coerce').fillna(1).astype(int)
    
    # TRANSFER íŠ¸ëœì­ì…˜ ë§ˆìŠ¤í¬
    transfer_mask = df[tx_col].str.contains('TRANSFER', na=False, case=False)
    
    if not transfer_mask.any():
        logger.info("TRANSFER íŠ¸ëœì­ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
        return df
    
    # TRANSFER íŠ¸ëœì­ì…˜ í”¼ë²— ë¶„ì„
    transfer_df = df[transfer_mask].copy()
    
    # ì¼€ì´ìŠ¤ë³„ TRANSFER IN/OUT ì§‘ê³„
    pivot_key = ['Case_No', 'Location', 'Target_Warehouse']
    
    try:
        pivot_table = transfer_df.pivot_table(
            index=pivot_key,
            columns=tx_col,
            values='Qty',
            aggfunc='sum',
            fill_value=0
        )
    except Exception as e:
        logger.warning(f"í”¼ë²— í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
        return df
    
    # INë§Œ ìˆëŠ” ì¼€ì´ìŠ¤ (OUT ìƒì„± í•„ìš”)
    transfer_in_col = 'TRANSFER_IN'
    transfer_out_col = 'TRANSFER_OUT'
    
    orphan_in = []
    orphan_out = []
    
    for idx, row in pivot_table.iterrows():
        case_no, location, target_wh = idx
        
        in_qty = row.get(transfer_in_col, 0)
        out_qty = row.get(transfer_out_col, 0)
        
        if in_qty > 0 and out_qty == 0:
            # INë§Œ ìˆìŒ -> OUT ìƒì„±
            orphan_in.append((case_no, location, target_wh, in_qty))
            
        elif out_qty > 0 and in_qty == 0:
            # OUTë§Œ ìˆìŒ -> IN ìƒì„±
            orphan_out.append((case_no, location, target_wh, out_qty))
    
    # ìˆ˜ì • ë ˆì½”ë“œ ìƒì„±
    fixes = []
    
    # INë§Œ ìˆëŠ” ê²½ìš° -> OUT ìƒì„±
    for case_no, location, target_wh, qty in orphan_in:
        # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ ê¸°ì¡´ ë‚ ì§œ ì°¸ì¡°
        case_dates = df[df['Case_No'] == case_no]['Date'].dropna()
        fix_date = case_dates.iloc[0] if len(case_dates) > 0 else pd.Timestamp.now()
        
        fix_record = {
            'Case_No': case_no,
            'Date': fix_date,
            'Qty': qty,
            tx_col: 'TRANSFER_OUT',
            'Location': location,  # ì¶œê³  ìœ„ì¹˜
            'Target_Warehouse': target_wh,
            'Loc_From': location,
            'Source_File': 'AUTO_FIX_IN_TO_OUT'
        }
        fixes.append(fix_record)
    
    # OUTë§Œ ìˆëŠ” ê²½ìš° -> IN ìƒì„± (692ê±´ í•´ê²°)
    for case_no, location, target_wh, qty in orphan_out:
        # í•´ë‹¹ ì¼€ì´ìŠ¤ì˜ ê¸°ì¡´ ë‚ ì§œ ì°¸ì¡°
        case_dates = df[df['Case_No'] == case_no]['Date'].dropna()
        fix_date = case_dates.iloc[0] if len(case_dates) > 0 else pd.Timestamp.now()
        
        fix_record = {
            'Case_No': case_no,
            'Date': fix_date,
            'Qty': qty,
            tx_col: 'TRANSFER_IN',
            'Location': target_wh,  # ì…ê³  ìœ„ì¹˜
            'Target_Warehouse': location,
            'Loc_From': location,
            'Source_File': 'AUTO_FIX_OUT_TO_IN'
        }
        fixes.append(fix_record)
    
    if fixes:
        print(f"ğŸ› ï¸ AUTO-FIX ì¶”ê°€: INâ†’OUT {len(orphan_in)}ê±´ / OUTâ†’IN {len(orphan_out)}ê±´")
        
        # ìˆ˜ì • ë ˆì½”ë“œë¥¼ DataFrameì— ì¶”ê°€
        fix_df = pd.DataFrame(fixes)
        
        # ì›ë³¸ DataFrameê³¼ ì»¬ëŸ¼ ë§ì¶”ê¸°
        for col in df.columns:
            if col not in fix_df.columns:
                fix_df[col] = 'AUTO_FIX' if col in ['Site', 'Source_File'] else None
        
        # ì»¬ëŸ¼ ìˆœì„œ ë§ì¶”ê¸°
        fix_df = fix_df.reindex(columns=df.columns, fill_value=None)
        
        # ê²°í•©
        result_df = pd.concat([df, fix_df], ignore_index=True)
        
        logger.info(f"âœ… TRANSFER ë³´ì • ì™„ë£Œ: {len(fixes)}ê±´ ì¶”ê°€")
        return result_df
    else:
        logger.info("âœ… TRANSFER ì§ì´ ì´ë¯¸ ì™„ì „í•¨")
        return df

def validate_transfer_pairs_fixed(df: pd.DataFrame) -> None:
    """
    TRANSFER ì§ ê²€ì¦ - ê°œì„ ëœ ë²„ì „
    """
    # íŠ¸ëœì­ì…˜ íƒ€ì… ì»¬ëŸ¼ í™•ì¸
    tx_col = None
    for candidate in ['TxType_Refined', 'Transaction_Type']:
        if candidate in df.columns:
            tx_col = candidate
            break
    
    if not tx_col:
        logger.warning("íŠ¸ëœì­ì…˜ íƒ€ì… ì»¬ëŸ¼ì´ ì—†ì–´ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return
    
    # TRANSFER ë§ˆìŠ¤í¬
    transfer_mask = df[tx_col].str.contains('TRANSFER', na=False, case=False)
    
    if not transfer_mask.any():
        logger.info("TRANSFER íŠ¸ëœì­ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
        return
    
    # ì¼€ì´ìŠ¤ë³„ TRANSFER IN/OUT ì§‘ê³„
    transfer_summary = (df[transfer_mask]
                       .groupby(['Case_No', tx_col])['Qty']
                       .sum()
                       .unstack(fill_value=0))
    
    # ì»¬ëŸ¼ í™•ì¸ ë° ìƒì„±
    if 'TRANSFER_IN' not in transfer_summary.columns:
        transfer_summary['TRANSFER_IN'] = 0
    if 'TRANSFER_OUT' not in transfer_summary.columns:
        transfer_summary['TRANSFER_OUT'] = 0
    
    # ì°¨ì´ ê³„ì‚°
    transfer_summary['DIFF'] = (
        transfer_summary['TRANSFER_IN'] - transfer_summary['TRANSFER_OUT']
    ).abs()
    
    # ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ í™•ì¸
    mismatched = transfer_summary[transfer_summary['DIFF'] > 0]
    
    if len(mismatched) > 0:
        print(f"\nâŒ TRANSFER ì§ ë¶ˆì¼ì¹˜: {len(mismatched)}ê±´")
        print("ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤ ìƒ˜í”Œ:")
        print(mismatched[['TRANSFER_IN', 'TRANSFER_OUT', 'DIFF']].head())
        raise ValueError(f"TRANSFER ì§ ë¶ˆì¼ì¹˜: {len(mismatched)} ì¼€ì´ìŠ¤")
    else:
        logger.info("âœ… TRANSFER ì§ ëª¨ë‘ ì¼ì¹˜")

def validate_date_sequence_fixed(df: pd.DataFrame) -> None:
    """
    ë‚ ì§œ ìˆœì„œ ê²€ì¦ - ê°œì„ ëœ ë²„ì „
    """
    case_col = 'Case_No' if 'Case_No' in df.columns else 'Case_ID'
    
    if case_col not in df.columns:
        logger.warning("ì¼€ì´ìŠ¤ ì»¬ëŸ¼ì´ ì—†ì–´ ë‚ ì§œ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤")
        return
    
    bad_cases = []
    
    for case_id, group in df.groupby(case_col):
        if len(group) <= 1:
            continue
            
        # ë‚ ì§œìˆœ ì •ë ¬
        sorted_dates = group.sort_values('Date')['Date']
        
        # ë‹¨ì¡° ì¦ê°€ í™•ì¸
        if not sorted_dates.is_monotonic_increasing:
            bad_cases.append(case_id)
    
    if bad_cases:
        print(f"âš ï¸ ë‚ ì§œ ì—­ìˆœ ì¼€ì´ìŠ¤: {len(bad_cases)}ê°œ")
        if len(bad_cases) <= 5:
            print(f"   ì˜ˆì‹œ: {bad_cases}")
        else:
            print(f"   ì˜ˆì‹œ: {bad_cases[:5]}... (ì´ {len(bad_cases)}ê°œ)")
        
        # AUTO_FIX ì¼€ì´ìŠ¤ëŠ” ê²½ê³ ë§Œ ì¶œë ¥
        auto_fix_cases = df[df.get('Source_File', '').str.contains('AUTO_FIX', na=False)]['Case_No'].unique()
        auto_fix_bad = [case for case in bad_cases if case in auto_fix_cases]
        
        if len(auto_fix_bad) == len(bad_cases):
            logger.warning("ëª¨ë“  ë‚ ì§œ ì—­ìˆœì´ AUTO_FIX ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤ - ë¬´ì‹œ")
            return
        
        raise ValueError(f"ë‚ ì§œ ì—­ìˆœ Case {len(bad_cases)}ê°œ")
    else:
        logger.info("âœ… ëª¨ë“  ì¼€ì´ìŠ¤ì˜ ë‚ ì§œ ìˆœì„œê°€ ì˜¬ë°”ë¦„")

class DeduplicationEngine:
    """HVDC ì´ì¤‘ê³„ì‚° ë°©ì§€ ì—”ì§„"""
    
    def __init__(self):
        self.seen_transactions = set()
        self.duplicate_log = []
        self.config = config_manager
        self.deduplication_rules = self.config.get_deduplication_config()
        
    def generate_transaction_hash(self, transaction: Dict) -> str:
        """íŠ¸ëœì­ì…˜ ê³ ìœ  í•´ì‹œ ìƒì„±"""
        data = transaction.get('data', {})
        
        # í•µì‹¬ ì‹ë³„ìë“¤
        key_fields = [
            str(data.get('warehouse', '')),
            str(data.get('site', '')),
            str(data.get('incoming', 0)),
            str(data.get('outgoing', 0)),
            str(data.get('date', '')),
        ]
        
        # í•´ì‹œ ìƒì„±
        key_string = '|'.join(key_fields)
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()
    
    def is_duplicate_transaction(self, transaction: Dict, existing_transactions: List[Dict]) -> Tuple[bool, Optional[Dict]]:
        """íŠ¸ëœì­ì…˜ ì¤‘ë³µ ì—¬ë¶€ íŒë‹¨"""
        current_data = transaction.get('data', {})
        
        for existing in existing_transactions:
            existing_data = existing.get('data', {})
            
            # 1. ìœ„ì¹˜ ì¼ì¹˜ í™•ì¸
            if not self._locations_match(current_data, existing_data):
                continue
                
            # 2. ì‹œê°„ ìœˆë„ìš° í™•ì¸
            if not self._within_time_window(current_data, existing_data):
                continue
                
            # 3. ìˆ˜ëŸ‰ ìœ ì‚¬ì„± í™•ì¸
            if self._quantities_similar(current_data, existing_data):
                return True, existing
                
        return False, None
    
    def _locations_match(self, data1: Dict, data2: Dict) -> bool:
        """ìœ„ì¹˜ ì •ë³´ ì¼ì¹˜ í™•ì¸"""
        location_fields = ['warehouse', 'site']
        
        for field in location_fields:
            val1 = str(data1.get(field, '')).strip().lower()
            val2 = str(data2.get(field, '')).strip().lower()
            
            if val1 and val2 and val1 != val2:
                return False
                
        return True
    
    def _within_time_window(self, data1: Dict, data2: Dict) -> bool:
        """ì‹œê°„ ìœˆë„ìš° ë‚´ í™•ì¸"""
        date1 = data1.get('date')
        date2 = data2.get('date')
        
        if not date1 or not date2:
            return True  # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ íŒë‹¨
            
        try:
            if isinstance(date1, str):
                date1 = pd.to_datetime(date1)
            if isinstance(date2, str):
                date2 = pd.to_datetime(date2)
                
            time_diff = abs((date1 - date2).total_seconds() / 60)
            time_window = self.deduplication_rules.get('time_window_minutes', 5)
            return time_diff <= time_window
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë¹„êµ ì‹¤íŒ¨: {e}")
            return True
    
    def _quantities_similar(self, data1: Dict, data2: Dict) -> bool:
        """ìˆ˜ëŸ‰ ìœ ì‚¬ì„± í™•ì¸"""
        quantity_fields = ['incoming', 'outgoing', 'inventory']
        tolerance = self.deduplication_rules.get('quantity_tolerance', 0.1)
        
        matches = 0
        total_fields = 0
        
        for field in quantity_fields:
            val1 = data1.get(field, 0)
            val2 = data2.get(field, 0)
            
            if val1 != 0 or val2 != 0:  # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ 0ì´ ì•„ë‹ˆë©´ ë¹„êµ
                total_fields += 1
                if abs(float(val1) - float(val2)) <= tolerance:
                    matches += 1
                    
        return matches > 0 and matches == total_fields
    
    def handle_internal_transfers(self, transactions: List[Dict]) -> List[Dict]:
        """ë‚´ë¶€ ì´ë™ ì²˜ë¦¬ - í•œ ë²ˆë§Œ ì°¨ê°í•˜ë„ë¡ ë³´ì¥"""
        logger.info("ğŸ”„ ë‚´ë¶€ ì´ë™ ì²˜ë¦¬ ì‹œì‘")
        
        internal_config = self.deduplication_rules.get('internal_transfer_handling', {})
        single_deduction = internal_config.get('single_deduction', True)
        internal_warehouses = internal_config.get('internal_warehouses', ['Shifting'])
        
        if not single_deduction:
            logger.info("ë‚´ë¶€ ì´ë™ ë‹¨ì¼ ì°¨ê° ë¹„í™œì„±í™”ë¨")
            return transactions
        
        # ë‚´ë¶€ ì°½ê³ ë³„ë¡œ ê·¸ë£¹í™”
        internal_groups = {}
        regular_transactions = []
        
        for transaction in transactions:
            data = transaction.get('data', {})
            warehouse = data.get('warehouse', '')
            
            if self.config.is_internal_warehouse(warehouse):
                if warehouse not in internal_groups:
                    internal_groups[warehouse] = []
                internal_groups[warehouse].append(transaction)
            else:
                regular_transactions.append(transaction)
        
        # ë‚´ë¶€ ì´ë™ ì²˜ë¦¬
        processed_internal = []
        for warehouse, group in internal_groups.items():
            logger.info(f"ë‚´ë¶€ ì°½ê³  ì²˜ë¦¬: {warehouse} ({len(group)}ê±´)")
            
            if len(group) == 1:
                processed_internal.append(group[0])
            else:
                # ì¤‘ë³µ ì œê±° í›„ ì²« ë²ˆì§¸ë§Œ ìœ ì§€
                deduplicated = self.remove_duplicates_for_internal(group)
                if deduplicated:
                    processed_internal.append(deduplicated[0])
                    logger.info(f"  âœ… {warehouse}: {len(group)}ê±´ â†’ 1ê±´ìœ¼ë¡œ í†µí•©")
        
        result = regular_transactions + processed_internal
        logger.info(f"âœ… ë‚´ë¶€ ì´ë™ ì²˜ë¦¬ ì™„ë£Œ: {len(transactions)}ê±´ â†’ {len(result)}ê±´")
        
        return result
    
    def remove_duplicates_for_internal(self, transactions: List[Dict]) -> List[Dict]:
        """ë‚´ë¶€ ì´ë™ìš© ì¤‘ë³µ ì œê±° (ë” ì—„ê²©í•œ ê¸°ì¤€)"""
        if len(transactions) <= 1:
            return transactions
        
        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_transactions = sorted(transactions, 
                                   key=lambda x: x.get('data', {}).get('date', datetime.min))
        
        # ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜ë§Œ ìœ ì§€
        return [sorted_transactions[0]]
    
    def remove_duplicates(self, transactions: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ íŠ¸ëœì­ì…˜ ì œê±°"""
        logger.info(f"ğŸ” ì¤‘ë³µ ì œê±° ì‹œì‘: {len(transactions)}ê±´")
        
        # ë‚´ë¶€ ì´ë™ ì²˜ë¦¬ ë¨¼ì € ìˆ˜í–‰
        if self.deduplication_rules.get('enable_deduplication', True):
            transactions = self.handle_internal_transfers(transactions)
        
        unique_transactions = []
        duplicate_count = 0
        
        for i, transaction in enumerate(transactions):
            is_duplicate, duplicate_of = self.is_duplicate_transaction(
                transaction, unique_transactions
            )
            
            if is_duplicate:
                duplicate_count += 1
                self.duplicate_log.append({
                    'index': i,
                    'transaction': transaction,
                    'duplicate_of': duplicate_of,
                    'reason': 'Similar transaction found'
                })
                logger.debug(f"ì¤‘ë³µ ë°œê²¬: {transaction['source_file']}")
            else:
                unique_transactions.append(transaction)
                
        logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicate_count}ê±´ ì œê±°, {len(unique_transactions)}ê±´ ìœ ì§€")
        return unique_transactions
    
    def remove_hash_duplicates(self, transactions: List[Dict]) -> List[Dict]:
        """í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ë¹ ë¥¸ ë°©ë²•)"""
        logger.info(f"ğŸ” í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: {len(transactions)}ê±´")
        
        seen_hashes = set()
        unique_transactions = []
        duplicate_count = 0
        
        for transaction in transactions:
            tx_hash = self.generate_transaction_hash(transaction)
            
            if tx_hash in seen_hashes:
                duplicate_count += 1
                self.duplicate_log.append({
                    'hash': tx_hash,
                    'transaction': transaction,
                    'reason': 'Identical hash'
                })
            else:
                seen_hashes.add(tx_hash)
                unique_transactions.append(transaction)
                
        logger.info(f"âœ… í•´ì‹œ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicate_count}ê±´ ì œê±°, {len(unique_transactions)}ê±´ ìœ ì§€")
        return unique_transactions
    
    def detect_logical_duplicates(self, transactions: List[Dict]) -> List[Dict]:
        """ë…¼ë¦¬ì  ì¤‘ë³µ ê°ì§€ ë° í†µí•©"""
        logger.info("ğŸ” ë…¼ë¦¬ì  ì¤‘ë³µ ê°ì§€ ì‹œì‘")
        
        # ì°½ê³ ë³„, ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
        grouped = self._group_transactions_by_key(transactions)
        
        merged_transactions = []
        merge_count = 0
        
        for key, group in grouped.items():
            if len(group) > 1:
                # ë™ì¼ í‚¤ì˜ íŠ¸ëœì­ì…˜ë“¤ ë³‘í•©
                merged = self._merge_transaction_group(group)
                merged_transactions.append(merged)
                merge_count += len(group) - 1
            else:
                merged_transactions.append(group[0])
                
        logger.info(f"âœ… ë…¼ë¦¬ì  ì¤‘ë³µ ì²˜ë¦¬ ì™„ë£Œ: {merge_count}ê±´ ë³‘í•©")
        return merged_transactions
    
    def _group_transactions_by_key(self, transactions: List[Dict]) -> Dict[str, List[Dict]]:
        """íŠ¸ëœì­ì…˜ì„ í‚¤ë³„ë¡œ ê·¸ë£¹í™”"""
        groups = {}
        
        for transaction in transactions:
            data = transaction.get('data', {})
            
            # ê·¸ë£¹í•‘ í‚¤ ìƒì„±
            key = f"{data.get('warehouse', '')}_" \
                  f"{data.get('site', '')}_" \
                  f"{str(data.get('date', ''))[:10]}"  # ë‚ ì§œë§Œ (ì‹œê°„ ì œì™¸)
            
            if key not in groups:
                groups[key] = []
            groups[key].append(transaction)
            
        return groups
    
    def _merge_transaction_group(self, transactions: List[Dict]) -> Dict:
        """ë™ì¼ ê·¸ë£¹ì˜ íŠ¸ëœì­ì…˜ë“¤ ë³‘í•©"""
        if len(transactions) == 1:
            return transactions[0]
            
        # ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜ì„ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
        merged = transactions[0].copy()
        merged_data = merged['data'].copy()
        
        # ìˆ˜ëŸ‰ í•„ë“œë“¤ í•©ì‚°
        quantity_fields = ['incoming', 'outgoing']
        for field in quantity_fields:
            total = sum(t['data'].get(field, 0) for t in transactions)
            if total > 0:
                merged_data[field] = total
                
        # ì†ŒìŠ¤ ì •ë³´ í†µí•©
        source_files = list(set(t['source_file'] for t in transactions))
        merged['source_file'] = '; '.join(source_files)
        
        # ë³‘í•© ë©”íƒ€ë°ì´í„° ì¶”ê°€
        merged['merged_from'] = len(transactions)
        merged['merged_at'] = datetime.now()
        merged['data'] = merged_data
        
        return merged
    
    def validate_deduplication_integrity(self, original: List[Dict], deduplicated: List[Dict]) -> Dict[str, Any]:
        """ì¤‘ë³µ ì œê±° ë¬´ê²°ì„± ê²€ì¦"""
        logger.info("ğŸ” ì¤‘ë³µ ì œê±° ë¬´ê²°ì„± ê²€ì¦")
        
        # ì „ì²´ ìˆ˜ëŸ‰ ë³´ì¡´ í™•ì¸
        original_totals = self._calculate_totals(original)
        deduplicated_totals = self._calculate_totals(deduplicated)
        
        integrity_report = {
            'original_count': len(original),
            'deduplicated_count': len(deduplicated),
            'removed_count': len(original) - len(deduplicated),
            'original_totals': original_totals,
            'deduplicated_totals': deduplicated_totals,
            'quantity_preserved': abs(original_totals['total_incoming'] - deduplicated_totals['total_incoming']) < 0.01,
            'duplicate_log_count': len(self.duplicate_log)
        }
        
        # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
        if integrity_report['quantity_preserved']:
            logger.info("âœ… ì¤‘ë³µ ì œê±° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
        else:
            logger.warning("âš ï¸ ì¤‘ë³µ ì œê±° í›„ ìˆ˜ëŸ‰ ë¶ˆì¼ì¹˜ ë°œê²¬")
            
        return integrity_report
    
    def _calculate_totals(self, transactions: List[Dict]) -> Dict[str, float]:
        """íŠ¸ëœì­ì…˜ ì´í•© ê³„ì‚°"""
        totals = {
            'total_incoming': 0,
            'total_outgoing': 0,
            'total_inventory': 0
        }
        
        for transaction in transactions:
            data = transaction.get('data', {})
            totals['total_incoming'] += data.get('incoming', 0)
            totals['total_outgoing'] += data.get('outgoing', 0)
            totals['total_inventory'] += data.get('inventory', 0)
            
        return totals
    
    def get_deduplication_report(self) -> Dict[str, Any]:
        """ì¤‘ë³µ ì œê±° ë¦¬í¬íŠ¸ ìƒì„±"""
        duplicate_sources = {}
        for dup in self.duplicate_log:
            source = dup['transaction']['source_file']
            if source not in duplicate_sources:
                duplicate_sources[source] = 0
            duplicate_sources[source] += 1
            
        return {
            'total_duplicates_found': len(self.duplicate_log),
            'duplicates_by_source': duplicate_sources,
            'deduplication_rules': self.deduplication_rules,
            'sample_duplicates': self.duplicate_log[:5]  # ì²˜ìŒ 5ê°œ ìƒ˜í”Œ
        }
    
    def apply_comprehensive_deduplication(self, transactions: List[Dict]) -> Tuple[List[Dict], Dict[str, Any]]:
        """ì¢…í•©ì  ì¤‘ë³µ ì œê±° íŒŒì´í”„ë¼ì¸"""
        logger.info("ğŸš€ ì¢…í•©ì  ì¤‘ë³µ ì œê±° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        original_count = len(transactions)
        
        # 1. í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ë¹ ë¥¸ ì œê±°)
        step1_result = self.remove_hash_duplicates(transactions)
        
        # 2. ë…¼ë¦¬ì  ì¤‘ë³µ ê°ì§€ ë° í†µí•©
        step2_result = self.detect_logical_duplicates(step1_result)
        
        # 3. ì„¸ë°€í•œ ì¤‘ë³µ ì œê±°
        final_result = self.remove_duplicates(step2_result)
        
        # 4. ë¬´ê²°ì„± ê²€ì¦
        integrity_report = self.validate_deduplication_integrity(transactions, final_result)
        
        # 5. ì¢…í•© ë¦¬í¬íŠ¸
        comprehensive_report = {
            'pipeline_steps': {
                'original': original_count,
                'after_hash_dedup': len(step1_result),
                'after_logical_merge': len(step2_result),
                'final': len(final_result)
            },
            'total_removed': original_count - len(final_result),
            'removal_rate': (original_count - len(final_result)) / original_count * 100,
            'integrity_check': integrity_report,
            'deduplication_report': self.get_deduplication_report()
        }
        
        logger.info(f"âœ… ì¢…í•© ì¤‘ë³µ ì œê±° ì™„ë£Œ: {original_count} â†’ {len(final_result)} ({comprehensive_report['removal_rate']:.1f}% ì œê±°)")
        
        return final_result, comprehensive_report

class InventoryEngine:
    """HVDC ì¬ê³  ì‚°ì¶œ ì—”ì§„ - ê°œì„ ëœ ë²„ì „"""
    
    def __init__(self):
        self.calculation_method = "USER_VALIDATED_LOOP"
        
    def calculate_daily_inventory_simplified(self, transaction_df: pd.DataFrame) -> pd.DataFrame:
        """
        ê°„ì†Œí™”ëœ ì¼ë³„ ì¬ê³  ê³„ì‚°
        ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ì¶˜ ì •í™•í•œ ê³„ì‚°
        """
        print("ğŸ“Š ì¼ë³„ ì¬ê³  ê³„ì‚° ì¤‘...")
        
        if transaction_df.empty:
            return pd.DataFrame()
        
        # ë‚ ì§œ ì •ê·œí™”
        transaction_df['Date'] = pd.to_datetime(transaction_df['Date']).dt.date
        
        # TxType_Refinedë³„ ì§‘ê³„
        daily_summary = transaction_df.groupby(['Location', 'Date', 'TxType_Refined']).agg({
            'Qty': 'sum'
        }).reset_index()
        
        # í”¼ë²—ìœ¼ë¡œ IN/OUT ë¶„ë¦¬
        daily_pivot = daily_summary.pivot_table(
            index=['Location', 'Date'],
            columns='TxType_Refined', 
            values='Qty',
            fill_value=0
        ).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        daily_pivot.columns.name = None
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ í™•ì¸ ë° ì¶”ê°€
        required_cols = ['IN', 'TRANSFER_OUT', 'FINAL_OUT']
        for col in required_cols:
            if col not in daily_pivot.columns:
                daily_pivot[col] = 0
        
        # ì¬ê³  ê³„ì‚° (ìœ„ì¹˜ë³„ ëˆ„ì )
        stock_records = []
        
        for location in daily_pivot['Location'].unique():
            if location in ['UNKNOWN', 'UNK', '']:
                continue
                
            # í•´ë‹¹ ìœ„ì¹˜ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
            loc_data = daily_pivot[daily_pivot['Location'] == location].copy()
            loc_data = loc_data.sort_values('Date').reset_index(drop=True)
            
            # ì´ˆê¸° ì¬ê³  (0ë¶€í„° ì‹œì‘)
            current_stock = 0
            
            for idx, row in loc_data.iterrows():
                # ì…ê³ ëŸ‰
                inbound = row.get('IN', 0)
                
                # ì¶œê³ ëŸ‰ (TRANSFER + FINAL)
                transfer_out = row.get('TRANSFER_OUT', 0)
                final_out = row.get('FINAL_OUT', 0)
                total_outbound = transfer_out + final_out
                
                # ì¬ê³  ê³„ì‚°: ì´ì „ì¬ê³  + ì…ê³  - ì¶œê³ 
                opening_stock = current_stock
                closing_stock = opening_stock + inbound - total_outbound
                current_stock = closing_stock  # ë‹¤ìŒ ë‚ ì„ ìœ„í•´ ì—…ë°ì´íŠ¸
                
                # ë ˆì½”ë“œ ìƒì„±
                stock_record = {
                    'Location': location,
                    'Date': row['Date'],
                    'Opening_Stock': opening_stock,
                    'Inbound': inbound,
                    'Transfer_Out': transfer_out,
                    'Final_Out': final_out,
                    'Total_Outbound': total_outbound,
                    'Closing_Stock': closing_stock
                }
                stock_records.append(stock_record)
        
        daily_stock_df = pd.DataFrame(stock_records)
        
        if not daily_stock_df.empty:
            print(f"âœ… {len(daily_stock_df)}ê°œ ì¼ë³„ ì¬ê³  ìŠ¤ëƒ…ìƒ· ìƒì„±")
            
            # ìµœì¢… ì¬ê³  ìš”ì•½ ì¶œë ¥
            latest_stock = (daily_stock_df
                           .sort_values('Date')
                           .groupby('Location')
                           .tail(1))
            
            print("\nğŸ“Š ìµœì¢… ì¬ê³  ìš”ì•½:")
            for _, row in latest_stock.iterrows():
                print(f"   {row['Location']}: {int(row['Closing_Stock'])} EA")
        else:
            print("âš ï¸ ì¬ê³  ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        return daily_stock_df
    
    def validate_stock_calculation(self, daily_stock: pd.DataFrame) -> Dict[str, Any]:
        """ì¬ê³  ê³„ì‚° ê²€ì¦"""
        if daily_stock.empty:
            return {"status": "EMPTY", "message": "ê²€ì¦í•  ë°ì´í„° ì—†ìŒ"}
        
        validation_results = {
            'total_records': len(daily_stock),
            'locations_count': daily_stock['Location'].nunique(),
            'date_range': {
                'start': daily_stock['Date'].min(),
                'end': daily_stock['Date'].max()
            },
            'negative_stock_count': 0,
            'validation_errors': []
        }
        
        # ìŒìˆ˜ ì¬ê³  í™•ì¸
        negative_stock = daily_stock[daily_stock['Closing_Stock'] < 0]
        validation_results['negative_stock_count'] = len(negative_stock)
        
        if len(negative_stock) > 0:
            validation_results['validation_errors'].append(
                f"ìŒìˆ˜ ì¬ê³  ë°œê²¬: {len(negative_stock)}ê±´"
            )
            print(f"âš ï¸ ìŒìˆ˜ ì¬ê³  ë°œê²¬: {len(negative_stock)}ê±´")
        
        # ì¬ê³  ë¬´ê²°ì„± í™•ì¸ (Opening + Inbound - Outbound = Closing)
        integrity_check = daily_stock.copy()
        integrity_check['Calculated_Closing'] = (
            integrity_check['Opening_Stock'] + 
            integrity_check['Inbound'] - 
            integrity_check['Total_Outbound']
        )
        integrity_check['Difference'] = (
            integrity_check['Closing_Stock'] - integrity_check['Calculated_Closing']
        ).abs()
        
        integrity_errors = integrity_check[integrity_check['Difference'] > 0.01]
        
        if len(integrity_errors) > 0:
            validation_results['validation_errors'].append(
                f"ì¬ê³  ë¬´ê²°ì„± ì˜¤ë¥˜: {len(integrity_errors)}ê±´"
            )
        else:
            validation_results['integrity_status'] = "PASS"
        
        return validation_results
    
    def generate_summary_report(self, daily_stock: pd.DataFrame) -> Dict[str, Any]:
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        if daily_stock.empty:
            return {}
        
        # ìµœì¢… ì¬ê³ 
        final_inventory = (daily_stock
                          .sort_values('Date')
                          .groupby('Location')
                          .tail(1))
        
        # ì´ ì…ì¶œê³ ëŸ‰
        total_summary = daily_stock.groupby('Location').agg({
            'Inbound': 'sum',
            'Transfer_Out': 'sum',
            'Final_Out': 'sum',
            'Total_Outbound': 'sum'
        }).reset_index()
        
        # ìµœì¢… ì¬ê³ ì™€ í•©ì¹˜ê¸°
        summary = total_summary.merge(
            final_inventory[['Location', 'Closing_Stock']], 
            on='Location', 
            how='left'
        )
        
        return {
            'generation_time': pd.Timestamp.now(),
            'calculation_method': self.calculation_method,
            'warehouse_summary': summary.to_dict('records'),
            'total_final_inventory': summary['Closing_Stock'].sum(),
            'total_inbound': summary['Inbound'].sum(),
            'total_outbound': summary['Total_Outbound'].sum(),
            'locations': summary['Location'].tolist()
        } 