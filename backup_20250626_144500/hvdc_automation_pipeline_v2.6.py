#!/usr/bin/env python3
"""
HVDC í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ v2.6
ìµœì‹  ì‹¤ë¬´ ê¸°ì¤€: í™•ì¥í˜• ë§¤í•‘ + ì˜¨í†¨ë¡œì§€ ì—°ê³„ + ìë™í™” ë¦¬í¬íŠ¸
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import logging

# í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from excel_reporter import generate_excel_comprehensive_report
    from ontology_mapper import dataframe_to_rdf
except ImportError as e:
    print(f"âš ï¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("í•„ìš” ëª¨ë“ˆ: excel_reporter, ontology_mapper")

class HVDCAutomationPipeline:
    """HVDC í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ v2.6"""
    
    def __init__(self, mapping_file: str = "mapping_rules_v2.6.json"):
        self.mapping_file = mapping_file
        self.logger = self._setup_logger()
        self.mapping_rules = self._load_mapping_rules()
        
    def _load_mapping_rules(self) -> dict:
        """í™•ì¥í˜• ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
        try:
            with open(self.mapping_file, 'r', encoding='utf-8') as f:
                rules = json.load(f)
            self.logger.info(f"âœ… ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì™„ë£Œ: {self.mapping_file}")
            return rules
        except Exception as e:
            self.logger.error(f"âŒ ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logging.basicConfig(level=logging.INFO)
        return logging.getLogger(__name__)
    
    def normalize_vendor(self, df: pd.DataFrame, vendor_col: str = "Vendor") -> pd.DataFrame:
        """ë²¤ë”ëª… í‘œì¤€í™” (mapping_rules ê¸°ë°˜)"""
        if vendor_col not in df.columns:
            return df
            
        vendor_mappings = self.mapping_rules.get("vendor_mappings", {})
        
        def normalize(val):
            if pd.isna(val): return 'Unknown'
            for k, std in vendor_mappings.items():
                if k in str(val).upper():
                    return std
            return str(val).upper()
        
        df[vendor_col] = df[vendor_col].apply(normalize)
        self.logger.info(f"âœ… ë²¤ë” í‘œì¤€í™” ì™„ë£Œ: {df[vendor_col].nunique()}ê°œ ê³ ìœ  ë²¤ë”")
        return df
    
    def standardize_container_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¨í…Œì´ë„ˆ ì»¬ëŸ¼ ê·¸ë£¹í•‘ (20FT/40FT/20FR/40FR)"""
        container_groups = self.mapping_rules.get("container_column_groups", {})
        
        for std_col, variants in container_groups.items():
            df[std_col] = 0  # initialize
            for var in variants:
                for col in df.columns:
                    if col.replace(" ", "").replace("-", "").upper() == var.replace(" ", "").replace("-", "").upper():
                        df[std_col] = df[std_col] + pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        self.logger.info(f"âœ… ì»¨í…Œì´ë„ˆ ê·¸ë£¹í•‘ ì™„ë£Œ: {list(container_groups.keys())}")
        return df
    
    def add_storage_type(self, df: pd.DataFrame, location_col: str = "Location") -> pd.DataFrame:
        """Storage_Type ìë™ë¶„ë¥˜"""
        if location_col not in df.columns:
            return df
            
        warehouse_classification = self.mapping_rules.get("warehouse_classification", {})
        
        def classify_storage_type(location):
            if pd.isna(location): return 'Unknown'
            location_str = str(location).strip()
            for storage_type, keywords in warehouse_classification.items():
                if any(keyword.lower() in location_str.lower() for keyword in keywords):
                    return storage_type
            return 'Unknown'
        
        df['Storage_Type'] = df[location_col].apply(classify_storage_type)
        self.logger.info(f"âœ… Storage_Type ë¶„ë¥˜ ì™„ë£Œ: {df['Storage_Type'].value_counts().to_dict()}")
        return df
    
    def process_logistics_data(self, input_file: str) -> pd.DataFrame:
        """ì „ì²´ ë¡œì§ìŠ¤ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        self.logger.info(f"ğŸš€ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {input_file}")
        
        # 1. ë°ì´í„° ë¡œë”©
        df = pd.read_excel(input_file)
        self.logger.info(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
        
        # 2. ë²¤ë” í‘œì¤€í™”
        df = self.normalize_vendor(df)
        
        # 3. ì»¨í…Œì´ë„ˆ ê·¸ë£¹í•‘
        df = self.standardize_container_columns(df)
        
        # 4. Storage_Type ë¶„ë¥˜
        df = self.add_storage_type(df)
        
        # 5. ë‚ ì§œ ì²˜ë¦¬
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df['ì›”'] = df['Date'].dt.strftime('%Y-%m')
        
        # 6. ìˆ«ìí˜• í•„ë“œ ì •ë¦¬
        numeric_columns = ['Amount', 'TOTAL', 'Handling In', 'Handling out', 'Unstuffing', 'Stuffing', 'folk lift', 'crane']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = (
                    df[col]
                    .astype(str)
                    .str.replace(',', '')
                    .replace('N/A', '0')
                    .fillna(0)
                    .astype(float)
                )
        
        self.logger.info(f"âœ… ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {len(df)}í–‰")
        return df
    
    def generate_comprehensive_report(self, df: pd.DataFrame, output_file: str = "HVDC_ìµœì¢…í†µí•©ë¦¬í¬íŠ¸_v2.6.xlsx") -> bool:
        """í†µí•© ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            self.logger.info(f"ğŸ“Š í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘: {output_file}")
            
            # Excel ë¦¬í¬íŠ¸ ìƒì„±
            generate_excel_comprehensive_report(
                transaction_df=df,
                daily_stock=pd.DataFrame(),  # í•„ìš”ì‹œ ì¼ë³„ì¬ê³  ì¶”ê°€
                output_file=output_file,
                debug=True
            )
            
            self.logger.info(f"âœ… Excel ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def convert_to_ontology(self, df: pd.DataFrame, output_path: str = "rdf_output/hvdc_v2.6.ttl") -> str:
        """ì˜¨í†¨ë¡œì§€ RDF ë³€í™˜"""
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            
            # RDF ë³€í™˜
            rdf_path = dataframe_to_rdf(df, output_path)
            
            if rdf_path:
                self.logger.info(f"âœ… RDF ë³€í™˜ ì™„ë£Œ: {rdf_path}")
                return rdf_path
            else:
                self.logger.warning("âš ï¸ RDF ë³€í™˜ ì‹¤íŒ¨ (rdflib ë¯¸ì„¤ì¹˜)")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ ì˜¨í†¨ë¡œì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def run_full_pipeline(self, input_file: str, output_file: str = "HVDC_ìµœì¢…í†µí•©ë¦¬í¬íŠ¸_v2.6.xlsx") -> dict:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = datetime.now()
        
        try:
            # 1. ë°ì´í„° ì²˜ë¦¬
            df = self.process_logistics_data(input_file)
            
            # 2. í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
            report_success = self.generate_comprehensive_report(df, output_file)
            
            # 3. ì˜¨í†¨ë¡œì§€ ë³€í™˜
            rdf_path = self.convert_to_ontology(df)
            
            # 4. ê²°ê³¼ ìš”ì•½
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            result = {
                'success': report_success,
                'input_file': input_file,
                'output_file': output_file,
                'rdf_path': rdf_path,
                'data_rows': len(df),
                'data_columns': len(df.columns),
                'duration_seconds': duration,
                'timestamp': end_time.isoformat()
            }
            
            self.logger.info(f"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {duration:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ HVDC í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ v2.6 ì‹œì‘")
    print("=" * 60)
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = HVDCAutomationPipeline()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    input_files = [
        "HVDC WAREHOUSE_INVOICE1.xlsx",
        "invoice_full.xlsx",
        "data/HVDC WAREHOUSE_HITACHI(HE).xlsx"
    ]
    
    for input_file in input_files:
        if Path(input_file).exists():
            print(f"ğŸ“„ ì…ë ¥ íŒŒì¼ ë°œê²¬: {input_file}")
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            result = pipeline.run_full_pipeline(input_file)
            
            if result['success']:
                print(f"âœ… ì„±ê³µ: {result['output_file']}")
                print(f"ğŸ“Š ë°ì´í„°: {result['data_rows']}í–‰, {result['data_columns']}ì»¬ëŸ¼")
                print(f"â±ï¸ ì†Œìš”ì‹œê°„: {result['duration_seconds']:.2f}ì´ˆ")
                if result['rdf_path']:
                    print(f"ğŸ”— RDF: {result['rdf_path']}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            
            break
    else:
        print("âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼:")
        for file in input_files:
            print(f"  - {file}")

if __name__ == "__main__":
    main() 