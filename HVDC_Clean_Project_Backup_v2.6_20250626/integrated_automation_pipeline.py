#!/usr/bin/env python3
"""
HVDC í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ - mapping_rules_v2.6.json ê¸°ë°˜

ìƒˆë¡œìš´ ì—…ë¬´í•„ë“œê°€ ì¶”ê°€ë˜ë©´ mapping_rulesì—ë§Œ í•œ ì¤„ ì¶”ê°€í•˜ë©´
ì§‘ê³„, í”¼ë²—, ì—‘ì…€ ë¦¬í¬íŠ¸, RDF, SPARQL ì¿¼ë¦¬ê¹Œì§€ ëª¨ë‘ ìë™ í™•ì¥ë¨
"""

import pandas as pd
import numpy as np
from datetime import datetime
import json
import sys
from pathlib import Path

# í•µì‹¬ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from config import load_expected_stock
except Exception as e:
    print(f"âš ï¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
    def load_expected_stock(as_of=None):
        return {}
        
from core.deduplication import drop_duplicate_transfers, reconcile_orphan_transfers
from core.loader import DataLoader
from excel_reporter import (
    generate_monthly_in_out_stock_report,
    normalize_location_column,
    generate_excel_comprehensive_report
)
from mapping_utils import (
    mapping_manager, 
    add_storage_type_to_dataframe,
    normalize_vendor, 
    standardize_container_columns
)

# ontology_mapper ì„í¬íŠ¸ (RDF ë³€í™˜ìš©)
try:
    from ontology_mapper import dataframe_to_rdf
except ImportError:
    print("âš ï¸ ontology_mapper ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. RDF ë³€í™˜ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
    def dataframe_to_rdf(df, output_path):
        print(f"RDF ë³€í™˜ ê±´ë„ˆëœ€: {output_path}")
        return output_path

def load_mapping_rules():
    """mapping_rules_v2.6.json ë¡œë“œ"""
    try:
        with open('mapping_rules_v2.6.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ mapping_rules_v2.6.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def apply_mapping_rules_to_dataframe(df, mapping_rules):
    """mapping_rules ê¸°ë°˜ìœ¼ë¡œ DataFrame ì „ì²˜ë¦¬ ë° í™•ì¥"""
    print("ğŸ”„ mapping_rules ê¸°ë°˜ DataFrame ì „ì²˜ë¦¬ ì¤‘...")
    
    # 1. í•„ìˆ˜ ì»¬ëŸ¼ ìƒì„± (ë‚ ì§œ/ì›”)
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df['ì›”'] = df['Date'].dt.strftime('%Y-%m')
    
    # 2. mapping_rulesì˜ field_mapì— ìˆëŠ” ëª¨ë“  í•„ë“œê°€ DataFrameì— ìˆëŠ”ì§€ í™•ì¸
    field_map = mapping_rules.get('field_map', {})
    for field_name, predicate in field_map.items():
        if field_name not in df.columns:
            print(f"  ğŸ“ í•„ë“œ ì¶”ê°€: {field_name} (ê¸°ë³¸ê°’: 0)")
            df[field_name] = 0
    
    # 3. ë²¤ë” í‘œì¤€í™” (vendor_mappings ì ìš©)
    if 'Vendor' in df.columns:
        df['Vendor'] = df['Vendor'].apply(normalize_vendor)
    
    # 4. ì»¨í…Œì´ë„ˆ ì»¬ëŸ¼ í‘œì¤€í™” (container_column_groups ì ìš©)
    df = standardize_container_columns(df)
    
    # 5. ì¹´í…Œê³ ë¦¬ ì •ê·œí™”
    if 'Category' in df.columns:
        df['Category'] = df['Category'].str.strip().str.title()
    
    # 6. Storage Type ì¶”ê°€
    df = add_storage_type_to_dataframe(df, "Location")
    
    print(f"âœ… DataFrame ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df.columns)}ê°œ ì»¬ëŸ¼")
    return df

def generate_comprehensive_reports(df, mapping_rules, output_dir="reports"):
    """í†µí•© ë¦¬í¬íŠ¸ ìƒì„± (mapping_rules ê¸°ë°˜ ìë™ í™•ì¥)"""
    print("ğŸ“Š í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # 1. ê¸°ë³¸ ì›”ë³„ IN/OUT/ì¬ê³  ë¦¬í¬íŠ¸
    in_df, out_df, stock_df = generate_monthly_in_out_stock_report(df)
    
    # 2. mapping_rules ê¸°ë°˜ ìë™ ì§‘ê³„ ë¦¬í¬íŠ¸ ìƒì„±
    field_map = mapping_rules.get('field_map', {})
    property_mappings = mapping_rules.get('property_mappings', {})
    
    # ìˆ«ìí˜• í•„ë“œë“¤ ìë™ ì§‘ê³„
    numeric_fields = []
    for field, props in property_mappings.items():
        if props.get('datatype') in ['xsd:decimal', 'xsd:integer'] and field in df.columns:
            numeric_fields.append(field)
    
    print(f"  ğŸ“ˆ ìë™ ì§‘ê³„ í•„ë“œ: {numeric_fields}")
    
    # 3. í†µí•© ì—‘ì…€ ë¦¬í¬íŠ¸ ìƒì„±
    excel_report_path = f"{output_dir}/HVDC_í†µí•©ìë™í™”ë¦¬í¬íŠ¸_{timestamp}.xlsx"
    
    with pd.ExcelWriter(excel_report_path, engine='xlsxwriter') as writer:
        # ê¸°ë³¸ ì‹œíŠ¸ë“¤
        in_df.to_excel(writer, sheet_name='01_ì›”ë³„IN_ì°½ê³ í˜„ì¥', index=False)
        out_df.to_excel(writer, sheet_name='02_ì›”ë³„OUT_ì°½ê³ í˜„ì¥', index=False)
        stock_df.to_excel(writer, sheet_name='03_ì›”ë³„ì¬ê³ _ì°½ê³ í˜„ì¥', index=False)
        
        # mapping_rules ê¸°ë°˜ ìë™ ì§‘ê³„ ì‹œíŠ¸ë“¤
        sheet_counter = 4
        
        # ì›”ë³„ ì§‘ê³„ (ê° ìˆ«ì í•„ë“œë³„)
        for field in numeric_fields:
            if field in df.columns and df[field].sum() > 0:
                monthly_agg = df.groupby('ì›”')[field].sum().reset_index()
                monthly_agg.columns = ['ì›”', f'ì›”ë³„{field}í•©ê³„']
                sheet_name = f'{sheet_counter:02d}_ì›”ë³„{field}'
                monthly_agg.to_excel(writer, sheet_name=sheet_name, index=False)
                sheet_counter += 1
                print(f"    âœ… {field} ì›”ë³„ ì§‘ê³„ ì™„ë£Œ")
        
        # ì°½ê³ ë³„ ì§‘ê³„ (ê° ìˆ«ì í•„ë“œë³„)
        for field in numeric_fields:
            if field in df.columns and df[field].sum() > 0:
                location_agg = df.groupby('Location')[field].sum().reset_index()
                location_agg.columns = ['ì°½ê³ /í˜„ì¥', f'ì´{field}í•©ê³„']
                location_agg = location_agg.sort_values(f'ì´{field}í•©ê³„', ascending=False)
                sheet_name = f'{sheet_counter:02d}_ì°½ê³ ë³„{field}'
                location_agg.to_excel(writer, sheet_name=sheet_name, index=False)
                sheet_counter += 1
                print(f"    âœ… {field} ì°½ê³ ë³„ ì§‘ê³„ ì™„ë£Œ")
        
        # í†µê³„ ìš”ì•½ ì‹œíŠ¸
        stats_data = []
        for field in numeric_fields:
            if field in df.columns:
                stats_data.append({
                    'í•„ë“œëª…': field,
                    'ì´í•©': df[field].sum(),
                    'í‰ê· ': df[field].mean(),
                    'ìµœëŒ€ê°’': df[field].max(),
                    'ìµœì†Œê°’': df[field].min(),
                    'í‘œì¤€í¸ì°¨': df[field].std()
                })
        
        if stats_data:
            stats_df = pd.DataFrame(stats_data)
            stats_df.to_excel(writer, sheet_name=f'{sheet_counter:02d}_í†µê³„ìš”ì•½', index=False)
            print(f"    âœ… í†µê³„ ìš”ì•½ ì™„ë£Œ")
    
    print(f"âœ… í†µí•© ì—‘ì…€ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {excel_report_path}")
    return excel_report_path

def generate_rdf_from_dataframe(df, mapping_rules, output_dir="rdf_output"):
    """DataFrameì„ RDFë¡œ ë³€í™˜ (mapping_rules ê¸°ë°˜)"""
    print("ğŸ”— RDF ë³€í™˜ ì¤‘...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    rdf_path = f"{output_dir}/hvdc_automation_{timestamp}.ttl"
    
    try:
        # ontology_mapperì˜ dataframe_to_rdf í•¨ìˆ˜ ì‚¬ìš©
        result_path = dataframe_to_rdf(df, rdf_path)
        print(f"âœ… RDF ë³€í™˜ ì™„ë£Œ: {result_path}")
        return result_path
    except Exception as e:
        print(f"âš ï¸ RDF ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

def generate_sparql_queries(mapping_rules, output_dir="sparql_queries"):
    """mapping_rules ê¸°ë°˜ SPARQL ì¿¼ë¦¬ ìƒì„±"""
    print("ğŸ” SPARQL ì¿¼ë¦¬ ìƒì„± ì¤‘...")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(output_dir).mkdir(exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    sparql_templates = mapping_rules.get('sparql_templates', {})
    namespace = mapping_rules.get('namespace', 'http://samsung.com/project-logistics#')
    
    queries = []
    
    # ê¸°ë³¸ í…œí”Œë¦¿ ì¿¼ë¦¬ë“¤
    for query_name, template in sparql_templates.items():
        query = template.format(namespace=namespace)
        queries.append({
            'name': query_name,
            'query': query,
            'description': f'ìë™ ìƒì„±ëœ {query_name} ì¿¼ë¦¬'
        })
    
    # mapping_rules ê¸°ë°˜ ë™ì  ì¿¼ë¦¬ ìƒì„±
    property_mappings = mapping_rules.get('property_mappings', {})
    numeric_fields = [field for field, props in property_mappings.items() 
                     if props.get('datatype') in ['xsd:decimal', 'xsd:integer']]
    
    # Handling Fee íŠ¹ë³„ ì¿¼ë¦¬
    if 'Handling Fee' in numeric_fields:
        handling_fee_query = f"""
PREFIX ex: <{namespace}>
SELECT ?month ?warehouse (SUM(?handlingFee) AS ?totalHandlingFee)
WHERE {{
    ?event rdf:type ex:TransportEvent ;
           ex:hasLocation ?warehouse ;
           ex:hasDate ?date ;
           ex:hasHandlingFee ?handlingFee .
    BIND(SUBSTR(STR(?date), 1, 7) AS ?month)
}}
GROUP BY ?month ?warehouse
ORDER BY ?month ?warehouse
"""
        queries.append({
            'name': 'handling_fee_monthly_warehouse',
            'query': handling_fee_query,
            'description': 'ì›”ë³„ ì°½ê³ ë³„ Handling Fee ì§‘ê³„'
        })
    
    # ì¿¼ë¦¬ íŒŒì¼ ì €ì¥
    sparql_file = f"{output_dir}/generated_queries_{timestamp}.sparql"
    with open(sparql_file, 'w', encoding='utf-8') as f:
        for query_info in queries:
            f.write(f"# {query_info['description']}\n")
            f.write(f"# Query: {query_info['name']}\n")
            f.write(query_info['query'])
            f.write("\n\n")
    
    print(f"âœ… SPARQL ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ: {sparql_file}")
    return sparql_file

def main():
    """í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ HVDC í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    try:
        # 1. mapping_rules ë¡œë“œ
        print("ğŸ“‹ mapping_rules_v2.6.json ë¡œë“œ ì¤‘...")
        mapping_rules = load_mapping_rules()
        if not mapping_rules:
            print("âŒ mapping_rules ë¡œë“œ ì‹¤íŒ¨!")
            return False
        
        print(f"âœ… mapping_rules ë¡œë“œ ì™„ë£Œ: v{mapping_rules.get('version', 'unknown')}")
        
        # 2. ë°ì´í„° ë¡œë”©
        print("\nğŸ“„ ë°ì´í„° ë¡œë”© ì¤‘...")
        loader = DataLoader()
        excel_files = loader.load_excel_files("data")
        
        if not excel_files:
            print("âŒ Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            return False
            
        raw_transactions = loader.extract_transactions(excel_files)
        print(f"ğŸ“Š ì´ {len(raw_transactions):,}ê±´ì˜ ì›ì‹œ íŠ¸ëœì­ì…˜ ìˆ˜ì§‘")

        # 3. DataFrame ë³€í™˜
        print("\nğŸ”„ DataFrame ë³€í™˜ ì¤‘...")
        transaction_df = transactions_to_dataframe(raw_transactions)
        print(f"âœ… {len(transaction_df)}ê±´ íŠ¸ëœì­ì…˜ ìƒì„±")
        
        # 4. mapping_rules ê¸°ë°˜ ì „ì²˜ë¦¬ ë° í™•ì¥
        transaction_df = apply_mapping_rules_to_dataframe(transaction_df, mapping_rules)
        
        # 5. ë°ì´í„° ì „ì²˜ë¦¬
        print("\nğŸ› ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        transaction_df = reconcile_orphan_transfers(transaction_df)
        transaction_df = drop_duplicate_transfers(transaction_df)
        print("âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
        
        # 6. í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
        excel_report_path = generate_comprehensive_reports(transaction_df, mapping_rules)
        
        # 7. RDF ë³€í™˜
        rdf_path = generate_rdf_from_dataframe(transaction_df, mapping_rules)
        
        # 8. SPARQL ì¿¼ë¦¬ ìƒì„±
        sparql_path = generate_sparql_queries(mapping_rules)
        
        # 9. ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("ğŸ‰ í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š DataFrame ì»¬ëŸ¼ ìˆ˜: {len(transaction_df.columns)}ê°œ")
        print(f"ğŸ“„ ì—‘ì…€ ë¦¬í¬íŠ¸: {excel_report_path}")
        if rdf_path:
            print(f"ğŸ”— RDF íŒŒì¼: {rdf_path}")
        print(f"ğŸ” SPARQL ì¿¼ë¦¬: {sparql_path}")
        
        # 10. mapping_rules í™•ì¥ ê°€ëŠ¥ì„± ì•ˆë‚´
        print("\nğŸ“‹ mapping_rules í™•ì¥ ê°€ì´ë“œ:")
        print("  - ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€: field_mapê³¼ property_mappingsì— í•œ ì¤„ ì¶”ê°€")
        print("  - ë²¤ë” ë§¤í•‘: vendor_mappingsì— ì¶”ê°€")
        print("  - ì»¨í…Œì´ë„ˆ ê·¸ë£¹: container_column_groupsì— ì¶”ê°€")
        print("  - SPARQL í…œí”Œë¦¿: sparql_templatesì— ì¶”ê°€")
        print("  â†’ ì½”ë“œ ìˆ˜ì • ì—†ì´ ìë™ìœ¼ë¡œ ëª¨ë“  ë¦¬í¬íŠ¸/RDF/ì¿¼ë¦¬ì— ë°˜ì˜ë¨!")
        
        return True
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def transactions_to_dataframe(transactions):
    """íŠ¸ëœì­ì…˜ ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
    data = []
    
    for tx in transactions:
        tx_data = tx.get('data', {})
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        case_id = extract_case_id(tx_data)
        warehouse = extract_warehouse(tx_data)
        date_val = extract_datetime(tx_data)
        
        # ìˆ˜ëŸ‰ ì²˜ë¦¬
        incoming = tx_data.get('incoming', 0) or 0
        outgoing = tx_data.get('outgoing', 0) or 0
        
        # ê¸°ë³¸ ë ˆì½”ë“œ í…œí”Œë¦¿
        base_record = {
            'Case_No': case_id,
            'Date': date_val,
            'Location': warehouse,
            'Source_File': tx.get('source_file', ''),
            'Loc_From': 'SOURCE',
            'Target_Warehouse': warehouse,
            'Amount': tx_data.get('amount', 0),
            'Handling Fee': tx_data.get('handling_fee', 0),  # mapping_rulesì— ì¶”ê°€ëœ í•„ë“œ
            'Storage_Type': tx_data.get('storage_type', 'Unknown'),
            'storage_type': tx_data.get('storage_type', 'Unknown')
        }
        
        # IN íŠ¸ëœì­ì…˜ ìƒì„±
        if incoming > 0:
            record = base_record.copy()
            record.update({
                'TxType_Refined': 'IN',
                'Qty': int(incoming),
                'Incoming': int(incoming),
                'Outgoing': 0
            })
            data.append(record)
            
        # OUT íŠ¸ëœì­ì…˜ ìƒì„±
        if outgoing > 0:
            record = base_record.copy()
            site = extract_site(warehouse)
            tx_type = 'FINAL_OUT' if site in ['AGI', 'DAS', 'MIR', 'SHU'] else 'TRANSFER_OUT'
                
            record.update({
                'TxType_Refined': tx_type,
                'Qty': int(outgoing),
                'Loc_From': warehouse,
                'Target_Warehouse': 'DESTINATION',
                'Incoming': 0,
                'Outgoing': int(outgoing)
            })
            data.append(record)
    
    df = pd.DataFrame(data)
    if not df.empty:
        df = normalize_location_column(df)
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
        df['Date'] = df['Date'].fillna(pd.Timestamp.now())
        df['Billing month'] = df['Date'].dt.strftime('%Y-%m')
        df['Category'] = 'General'
        
    return df

def extract_case_id(data):
    """ì¼€ì´ìŠ¤ ID ì¶”ì¶œ"""
    case_fields = ['case', 'Case', 'case_id', 'CaseID', 'ID', 'carton', 'box', 'mr#']
    
    for field in case_fields:
        if field in data and data[field]:
            case_value = str(data[field]).strip()
            if case_value and case_value.lower() not in ['nan', 'none', '']:
                return case_value
    
    return f"CASE_{abs(hash(str(data))) % 100000}"

def extract_warehouse(data):
    """ì°½ê³ ëª… ì¶”ì¶œ ë° ì •ê·œí™”"""
    warehouse_fields = ['warehouse', 'Warehouse', 'site', 'Site', 'location', 'Location']
    
    for field in warehouse_fields:
        if field in data and data[field]:
            raw_warehouse = str(data[field]).strip()
            if raw_warehouse and raw_warehouse.lower() not in ['nan', 'none', '']:
                return normalize_warehouse_name(raw_warehouse)
    
    return 'UNKNOWN'

def extract_datetime(data):
    """ë‚ ì§œ/ì‹œê°„ ì¶”ì¶œ"""
    date_fields = ['date', 'Date', 'timestamp', 'Timestamp', 'datetime']
    
    for field in date_fields:
        if field in data and data[field]:
            try:
                date_value = data[field]
                if isinstance(date_value, str) and date_value.lower() in ['nan', 'none', '']:
                    continue
                return pd.to_datetime(date_value)
            except:
                continue
    
    return pd.Timestamp.now()

def normalize_warehouse_name(raw_name):
    """ì°½ê³ ëª… ì •ê·œí™”"""
    if pd.isna(raw_name) or not raw_name:
        return 'UNKNOWN'
    
    all_locations = []
    for locations in mapping_manager.warehouse_classification.values():
        all_locations.extend(locations)
    
    name_lower = str(raw_name).lower().strip()
    
    for location in all_locations:
        if location.lower() in name_lower or name_lower in location.lower():
            return location
    
    return str(raw_name).strip()

def extract_site(warehouse_name):
    """ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ"""
    if pd.isna(warehouse_name):
        return 'UNK'
    
    if mapping_manager.classify_storage_type(warehouse_name) == 'Site':
        return warehouse_name
    
    return 'UNK'

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ í†µí•© ìë™í™” íŒŒì´í”„ë¼ì¸ ì„±ê³µ!")
        sys.exit(0)
    else:
        print("\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨!")
        sys.exit(1) 