"""
HVDC ì´ì¤‘ê³„ì‚° ë°©ì§€ ëª¨ë“ˆ
íŠ¸ëœì­ì…˜ ì¤‘ë³µ ì œê±° ë° ë¬´ê²°ì„± ë³´ì¥
"""

import pandas as pd
from typing import List, Dict, Any, Set, Tuple, Optional
import hashlib
import logging
from datetime import datetime, timedelta

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeduplicationEngine:
    """HVDC ì´ì¤‘ê³„ì‚° ë°©ì§€ ì—”ì§„"""
    
    def __init__(self):
        self.seen_transactions = set()
        self.duplicate_log = []
        self.deduplication_rules = {
            'time_window_minutes': 5,  # 5ë¶„ ì´ë‚´ ë™ì¼ íŠ¸ëœì­ì…˜ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
            'quantity_tolerance': 0.01,  # ìˆ˜ëŸ‰ ì°¨ì´ í—ˆìš© ë²”ìœ„
            'location_strict': True,  # ìœ„ì¹˜ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨
        }
        
    def generate_transaction_hash(self, transaction: Dict) -> str:
        """íŠ¸ëœì­ì…˜ ê³ ìœ  í•´ì‹œ ìƒì„±"""
        data = transaction.get('data', {})
        
        # í•µì‹¬ ì‹ë³„ìë“¤
        key_fields = [
            str(data.get('warehouse', '')),
            str(data.get('site', '')),
            str(data.get('incoming', 0)),
            str(data.get('outgoing', 0)),
            str(data.get('date', '')),
        ]
        
        # í•´ì‹œ ìƒì„±
        key_string = '|'.join(key_fields)
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()
    
    def is_duplicate_transaction(self, transaction: Dict, existing_transactions: List[Dict]) -> Tuple[bool, Optional[Dict]]:
        """íŠ¸ëœì­ì…˜ ì¤‘ë³µ ì—¬ë¶€ íŒë‹¨"""
        current_data = transaction.get('data', {})
        
        for existing in existing_transactions:
            existing_data = existing.get('data', {})
            
            # 1. ìœ„ì¹˜ ì¼ì¹˜ í™•ì¸
            if not self._locations_match(current_data, existing_data):
                continue
                
            # 2. ì‹œê°„ ìœˆë„ìš° í™•ì¸
            if not self._within_time_window(current_data, existing_data):
                continue
                
            # 3. ìˆ˜ëŸ‰ ìœ ì‚¬ì„± í™•ì¸
            if self._quantities_similar(current_data, existing_data):
                return True, existing
                
        return False, None
    
    def _locations_match(self, data1: Dict, data2: Dict) -> bool:
        """ìœ„ì¹˜ ì •ë³´ ì¼ì¹˜ í™•ì¸"""
        location_fields = ['warehouse', 'site']
        
        for field in location_fields:
            val1 = str(data1.get(field, '')).strip().lower()
            val2 = str(data2.get(field, '')).strip().lower()
            
            if val1 and val2 and val1 != val2:
                return False
                
        return True
    
    def _within_time_window(self, data1: Dict, data2: Dict) -> bool:
        """ì‹œê°„ ìœˆë„ìš° ë‚´ í™•ì¸"""
        date1 = data1.get('date')
        date2 = data2.get('date')
        
        if not date1 or not date2:
            return True  # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ íŒë‹¨
            
        try:
            if isinstance(date1, str):
                date1 = pd.to_datetime(date1)
            if isinstance(date2, str):
                date2 = pd.to_datetime(date2)
                
            time_diff = abs((date1 - date2).total_seconds() / 60)
            return time_diff <= self.deduplication_rules['time_window_minutes']
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë¹„êµ ì‹¤íŒ¨: {e}")
            return True
    
    def _quantities_similar(self, data1: Dict, data2: Dict) -> bool:
        """ìˆ˜ëŸ‰ ìœ ì‚¬ì„± í™•ì¸"""
        quantity_fields = ['incoming', 'outgoing', 'inventory']
        tolerance = self.deduplication_rules['quantity_tolerance']
        
        matches = 0
        total_fields = 0
        
        for field in quantity_fields:
            val1 = data1.get(field, 0)
            val2 = data2.get(field, 0)
            
            if val1 != 0 or val2 != 0:  # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ 0ì´ ì•„ë‹ˆë©´ ë¹„êµ
                total_fields += 1
                if abs(float(val1) - float(val2)) <= tolerance:
                    matches += 1
                    
        return matches > 0 and matches == total_fields
    
    def remove_duplicates(self, transactions: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ íŠ¸ëœì­ì…˜ ì œê±°"""
        logger.info(f"ğŸ” ì¤‘ë³µ ì œê±° ì‹œì‘: {len(transactions)}ê±´")
        
        unique_transactions = []
        duplicate_count = 0
        
        for i, transaction in enumerate(transactions):
            is_duplicate, duplicate_of = self.is_duplicate_transaction(
                transaction, unique_transactions
            )
            
            if is_duplicate:
                duplicate_count += 1
                self.duplicate_log.append({
                    'index': i,
                    'transaction': transaction,
                    'duplicate_of': duplicate_of,
                    'reason': 'Similar transaction found'
                })
                logger.debug(f"ì¤‘ë³µ ë°œê²¬: {transaction['source_file']}")
            else:
                unique_transactions.append(transaction)
                
        logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicate_count}ê±´ ì œê±°, {len(unique_transactions)}ê±´ ìœ ì§€")
        return unique_transactions
    
    def remove_hash_duplicates(self, transactions: List[Dict]) -> List[Dict]:
        """í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ë¹ ë¥¸ ë°©ë²•)"""
        logger.info(f"ğŸ” í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°: {len(transactions)}ê±´")
        
        seen_hashes = set()
        unique_transactions = []
        duplicate_count = 0
        
        for transaction in transactions:
            tx_hash = self.generate_transaction_hash(transaction)
            
            if tx_hash in seen_hashes:
                duplicate_count += 1
                self.duplicate_log.append({
                    'hash': tx_hash,
                    'transaction': transaction,
                    'reason': 'Identical hash'
                })
            else:
                seen_hashes.add(tx_hash)
                unique_transactions.append(transaction)
                
        logger.info(f"âœ… í•´ì‹œ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicate_count}ê±´ ì œê±°, {len(unique_transactions)}ê±´ ìœ ì§€")
        return unique_transactions
    
    def detect_logical_duplicates(self, transactions: List[Dict]) -> List[Dict]:
        """ë…¼ë¦¬ì  ì¤‘ë³µ ê°ì§€ ë° í†µí•©"""
        logger.info("ğŸ” ë…¼ë¦¬ì  ì¤‘ë³µ ê°ì§€ ì‹œì‘")
        
        # ì°½ê³ ë³„, ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”
        grouped = self._group_transactions_by_key(transactions)
        
        merged_transactions = []
        merge_count = 0
        
        for key, group in grouped.items():
            if len(group) > 1:
                # ë™ì¼ í‚¤ì˜ íŠ¸ëœì­ì…˜ë“¤ ë³‘í•©
                merged = self._merge_transaction_group(group)
                merged_transactions.append(merged)
                merge_count += len(group) - 1
            else:
                merged_transactions.append(group[0])
                
        logger.info(f"âœ… ë…¼ë¦¬ì  ì¤‘ë³µ ì²˜ë¦¬ ì™„ë£Œ: {merge_count}ê±´ ë³‘í•©")
        return merged_transactions
    
    def _group_transactions_by_key(self, transactions: List[Dict]) -> Dict[str, List[Dict]]:
        """íŠ¸ëœì­ì…˜ì„ í‚¤ë³„ë¡œ ê·¸ë£¹í™”"""
        groups = {}
        
        for transaction in transactions:
            data = transaction.get('data', {})
            
            # ê·¸ë£¹í•‘ í‚¤ ìƒì„±
            key = f"{data.get('warehouse', '')}_" \
                  f"{data.get('site', '')}_" \
                  f"{str(data.get('date', ''))[:10]}"  # ë‚ ì§œë§Œ (ì‹œê°„ ì œì™¸)
            
            if key not in groups:
                groups[key] = []
            groups[key].append(transaction)
            
        return groups
    
    def _merge_transaction_group(self, transactions: List[Dict]) -> Dict:
        """ë™ì¼ ê·¸ë£¹ì˜ íŠ¸ëœì­ì…˜ë“¤ ë³‘í•©"""
        if len(transactions) == 1:
            return transactions[0]
            
        # ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜ì„ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
        merged = transactions[0].copy()
        merged_data = merged['data'].copy()
        
        # ìˆ˜ëŸ‰ í•„ë“œë“¤ í•©ì‚°
        quantity_fields = ['incoming', 'outgoing']
        for field in quantity_fields:
            total = sum(t['data'].get(field, 0) for t in transactions)
            if total > 0:
                merged_data[field] = total
                
        # ì†ŒìŠ¤ ì •ë³´ í†µí•©
        source_files = list(set(t['source_file'] for t in transactions))
        merged['source_file'] = '; '.join(source_files)
        
        # ë³‘í•© ë©”íƒ€ë°ì´í„° ì¶”ê°€
        merged['merged_from'] = len(transactions)
        merged['merged_at'] = datetime.now()
        merged['data'] = merged_data
        
        return merged
    
    def validate_deduplication_integrity(self, original: List[Dict], deduplicated: List[Dict]) -> Dict[str, Any]:
        """ì¤‘ë³µ ì œê±° ë¬´ê²°ì„± ê²€ì¦"""
        logger.info("ğŸ” ì¤‘ë³µ ì œê±° ë¬´ê²°ì„± ê²€ì¦")
        
        # ì „ì²´ ìˆ˜ëŸ‰ ë³´ì¡´ í™•ì¸
        original_totals = self._calculate_totals(original)
        deduplicated_totals = self._calculate_totals(deduplicated)
        
        integrity_report = {
            'original_count': len(original),
            'deduplicated_count': len(deduplicated),
            'removed_count': len(original) - len(deduplicated),
            'original_totals': original_totals,
            'deduplicated_totals': deduplicated_totals,
            'quantity_preserved': abs(original_totals['total_incoming'] - deduplicated_totals['total_incoming']) < 0.01,
            'duplicate_log_count': len(self.duplicate_log)
        }
        
        # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
        if integrity_report['quantity_preserved']:
            logger.info("âœ… ì¤‘ë³µ ì œê±° ë¬´ê²°ì„± ê²€ì¦ í†µê³¼")
        else:
            logger.warning("âš ï¸ ì¤‘ë³µ ì œê±° í›„ ìˆ˜ëŸ‰ ë¶ˆì¼ì¹˜ ë°œê²¬")
            
        return integrity_report
    
    def _calculate_totals(self, transactions: List[Dict]) -> Dict[str, float]:
        """íŠ¸ëœì­ì…˜ ì´í•© ê³„ì‚°"""
        totals = {
            'total_incoming': 0,
            'total_outgoing': 0,
            'total_inventory': 0
        }
        
        for transaction in transactions:
            data = transaction.get('data', {})
            totals['total_incoming'] += data.get('incoming', 0)
            totals['total_outgoing'] += data.get('outgoing', 0)
            totals['total_inventory'] += data.get('inventory', 0)
            
        return totals
    
    def get_deduplication_report(self) -> Dict[str, Any]:
        """ì¤‘ë³µ ì œê±° ë¦¬í¬íŠ¸ ìƒì„±"""
        duplicate_sources = {}
        for dup in self.duplicate_log:
            source = dup['transaction']['source_file']
            if source not in duplicate_sources:
                duplicate_sources[source] = 0
            duplicate_sources[source] += 1
            
        return {
            'total_duplicates_found': len(self.duplicate_log),
            'duplicates_by_source': duplicate_sources,
            'deduplication_rules': self.deduplication_rules,
            'sample_duplicates': self.duplicate_log[:5]  # ì²˜ìŒ 5ê°œ ìƒ˜í”Œ
        }
    
    def apply_comprehensive_deduplication(self, transactions: List[Dict]) -> Tuple[List[Dict], Dict[str, Any]]:
        """ì¢…í•©ì  ì¤‘ë³µ ì œê±° íŒŒì´í”„ë¼ì¸"""
        logger.info("ğŸš€ ì¢…í•©ì  ì¤‘ë³µ ì œê±° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        original_count = len(transactions)
        
        # 1. í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ë¹ ë¥¸ ì œê±°)
        step1_result = self.remove_hash_duplicates(transactions)
        
        # 2. ë…¼ë¦¬ì  ì¤‘ë³µ ê°ì§€ ë° í†µí•©
        step2_result = self.detect_logical_duplicates(step1_result)
        
        # 3. ì„¸ë°€í•œ ì¤‘ë³µ ì œê±°
        final_result = self.remove_duplicates(step2_result)
        
        # 4. ë¬´ê²°ì„± ê²€ì¦
        integrity_report = self.validate_deduplication_integrity(transactions, final_result)
        
        # 5. ì¢…í•© ë¦¬í¬íŠ¸
        comprehensive_report = {
            'pipeline_steps': {
                'original': original_count,
                'after_hash_dedup': len(step1_result),
                'after_logical_merge': len(step2_result),
                'final': len(final_result)
            },
            'total_removed': original_count - len(final_result),
            'removal_rate': (original_count - len(final_result)) / original_count * 100,
            'integrity_check': integrity_report,
            'deduplication_report': self.get_deduplication_report()
        }
        
        logger.info(f"âœ… ì¢…í•© ì¤‘ë³µ ì œê±° ì™„ë£Œ: {original_count} â†’ {len(final_result)} ({comprehensive_report['removal_rate']:.1f}% ì œê±°)")
        
        return final_result, comprehensive_report 