"""
HVDC ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ëª¨ë“ˆ
ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ì •ê·œí™” ë° ë°ì´í„° í†µí•© ì²˜ë¦¬
"""

import pandas as pd
import os
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataLoader:
    """HVDC ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, mapping_rules_path: str = "mapping_rules_v2.4.json"):
        """
        Args:
            mapping_rules_path: ì˜¨í†¨ë¡œì§€ ë§¤í•‘ ê·œì¹™ íŒŒì¼ ê²½ë¡œ
        """
        self.mapping_rules_path = mapping_rules_path
        self.mapping_rules = self._load_mapping_rules()
        self.raw_data = {}
        self.processed_data = {}
        
    def _load_mapping_rules(self) -> Dict[str, Any]:
        """ë§¤í•‘ ê·œì¹™ ë¡œë“œ"""
        try:
            with open(self.mapping_rules_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ë§¤í•‘ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def load_excel_files(self, data_dir: str = "data") -> Dict[str, pd.DataFrame]:
        """Excel íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        excel_files = {}
        
        if not os.path.exists(data_dir):
            logger.error(f"ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {data_dir}")
            return excel_files
            
        for filename in os.listdir(data_dir):
            if filename.endswith('.xlsx'):
                file_path = os.path.join(data_dir, filename)
                try:
                    # Excel íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ ë¡œë“œ
                    excel_file = pd.read_excel(file_path, sheet_name=None, engine='openpyxl')
                    excel_files[filename] = excel_file
                    logger.info(f"ë¡œë“œ ì™„ë£Œ: {filename} ({len(excel_file)} ì‹œíŠ¸)")
                except Exception as e:
                    logger.error(f"Excel íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {filename}: {e}")
                    
        return excel_files
    
    def normalize_location_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìœ„ì¹˜ëª… ì •ê·œí™” (ì˜¨í†¨ë¡œì§€ ê¸°ë°˜)"""
        if 'location_mappings' not in self.mapping_rules:
            return df
            
        location_mappings = self.mapping_rules['location_mappings']
        
        # ìœ„ì¹˜ ê´€ë ¨ ì»¬ëŸ¼ë“¤ ì •ê·œí™”
        location_columns = ['Warehouse', 'Site', 'Location', 'From', 'To']
        
        for col in location_columns:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: self._normalize_single_location(x, location_mappings))
                
        return df
    
    def _normalize_single_location(self, location: str, mappings: Dict) -> str:
        """ë‹¨ì¼ ìœ„ì¹˜ëª… ì •ê·œí™”"""
        if pd.isna(location):
            return location
            
        location_str = str(location).strip()
        
        # ì§ì ‘ ë§¤í•‘ í™•ì¸
        for standard_name, variants in mappings.items():
            if location_str in variants or location_str == standard_name:
                return standard_name
                
        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
        for standard_name, variants in mappings.items():
            for variant in variants:
                if variant.lower() in location_str.lower() or location_str.lower() in variant.lower():
                    return standard_name
                    
        return location_str
    
    def extract_transactions(self, excel_files: Dict) -> List[Dict]:
        """Excel íŒŒì¼ë“¤ì—ì„œ íŠ¸ëœì­ì…˜ ë°ì´í„° ì¶”ì¶œ"""
        all_transactions = []
        
        for filename, sheets in excel_files.items():
            for sheet_name, df in sheets.items():
                try:
                    # ì‹œíŠ¸ë³„ íŠ¸ëœì­ì…˜ ì¶”ì¶œ
                    transactions = self._extract_sheet_transactions(df, filename, sheet_name)
                    all_transactions.extend(transactions)
                    logger.info(f"íŠ¸ëœì­ì…˜ ì¶”ì¶œ: {filename}/{sheet_name} - {len(transactions)}ê±´")
                except Exception as e:
                    logger.error(f"íŠ¸ëœì­ì…˜ ì¶”ì¶œ ì‹¤íŒ¨ {filename}/{sheet_name}: {e}")
                    
        return all_transactions
    
    def _extract_sheet_transactions(self, df: pd.DataFrame, filename: str, sheet_name: str) -> List[Dict]:
        """ë‹¨ì¼ ì‹œíŠ¸ì—ì„œ íŠ¸ëœì­ì…˜ ì¶”ì¶œ"""
        transactions = []
        
        if df.empty:
            return transactions
            
        # ì»¬ëŸ¼ëª… ì •ê·œí™”
        df = self._normalize_column_names(df)
        
        # ìœ„ì¹˜ëª… ì •ê·œí™”
        df = self.normalize_location_names(df)
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì‹ë³„ ë° ì²˜ë¦¬
        date_columns = self._identify_date_columns(df)
        
        for idx, row in df.iterrows():
            try:
                transaction = self._create_transaction_record(row, filename, sheet_name, date_columns)
                if transaction:
                    transactions.append(transaction)
            except Exception as e:
                logger.warning(f"íŠ¸ëœì­ì…˜ ìƒì„± ì‹¤íŒ¨ {filename}/{sheet_name} row {idx}: {e}")
                
        return transactions
    
    def _normalize_column_names(self, df: pd.DataFrame) -> pd.DataFrame:
        """ì»¬ëŸ¼ëª… ì •ê·œí™”"""
        column_mappings = {
            'incoming': ['Incoming', 'In', 'Inbound', 'ì…ê³ ', 'ì…ê³ ëŸ‰'],
            'outgoing': ['Outgoing', 'Out', 'Outbound', 'ì¶œê³ ', 'ì¶œê³ ëŸ‰'],
            'inventory': ['Inventory', 'Stock', 'ì¬ê³ ', 'ì¬ê³ ëŸ‰'],
            'date': ['Date', 'Timestamp', 'ë‚ ì§œ', 'ì¼ì'],
            'warehouse': ['Warehouse', 'WH', 'ì°½ê³ '],
            'site': ['Site', 'Location', 'ì‚¬ì´íŠ¸', 'ìœ„ì¹˜']
        }
        
        new_columns = {}
        for standard_name, variants in column_mappings.items():
            for col in df.columns:
                if any(variant.lower() in str(col).lower() for variant in variants):
                    new_columns[col] = standard_name
                    break
                    
        if new_columns:
            df = df.rename(columns=new_columns)
            
        return df
    
    def _identify_date_columns(self, df: pd.DataFrame) -> List[str]:
        """ë‚ ì§œ ì»¬ëŸ¼ ì‹ë³„"""
        date_columns = []
        
        for col in df.columns:
            if 'date' in str(col).lower() or 'time' in str(col).lower():
                date_columns.append(col)
            elif df[col].dtype == 'datetime64[ns]':
                date_columns.append(col)
                
        return date_columns
    
    def _create_transaction_record(self, row: pd.Series, filename: str, sheet_name: str, date_columns: List[str]) -> Optional[Dict]:
        """íŠ¸ëœì­ì…˜ ë ˆì½”ë“œ ìƒì„±"""
        transaction = {
            'source_file': filename,
            'source_sheet': sheet_name,
            'timestamp': datetime.now(),
            'data': {}
        }
        
        # ê¸°ë³¸ í•„ë“œ ì¶”ì¶œ
        essential_fields = ['incoming', 'outgoing', 'inventory', 'warehouse', 'site']
        has_essential_data = False
        
        for field in essential_fields:
            if field in row.index and pd.notna(row[field]):
                transaction['data'][field] = row[field]
                if field in ['incoming', 'outgoing', 'inventory'] and row[field] != 0:
                    has_essential_data = True
                    
        # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        for date_col in date_columns:
            if date_col in row.index and pd.notna(row[date_col]):
                transaction['data']['date'] = row[date_col]
                break
                
        # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        for col in row.index:
            if col not in transaction['data'] and pd.notna(row[col]):
                transaction['data'][col] = row[col]
                
        return transaction if has_essential_data else None
    
    def load_and_process_files(self, data_dir: str = "data") -> List[Dict]:
        """ì „ì²´ ë°ì´í„° ë¡œë”© ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        logger.info("ğŸš€ HVDC ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œì‘")
        
        # 1. Excel íŒŒì¼ë“¤ ë¡œë“œ
        excel_files = self.load_excel_files(data_dir)
        logger.info(f"Excel íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(excel_files)}ê°œ")
        
        # 2. íŠ¸ëœì­ì…˜ ì¶”ì¶œ
        transactions = self.extract_transactions(excel_files)
        logger.info(f"íŠ¸ëœì­ì…˜ ì¶”ì¶œ ì™„ë£Œ: {len(transactions)}ê±´")
        
        # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        validated_transactions = self._validate_transactions(transactions)
        logger.info(f"ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {len(validated_transactions)}ê±´")
        
        self.raw_data = excel_files
        self.processed_data = validated_transactions
        
        return validated_transactions
    
    def _validate_transactions(self, transactions: List[Dict]) -> List[Dict]:
        """íŠ¸ëœì­ì…˜ ë°ì´í„° ê²€ì¦"""
        validated = []
        
        for transaction in transactions:
            if self._is_valid_transaction(transaction):
                validated.append(transaction)
            else:
                logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ íŠ¸ëœì­ì…˜: {transaction}")
                
        return validated
    
    def _is_valid_transaction(self, transaction: Dict) -> bool:
        """íŠ¸ëœì­ì…˜ ìœ íš¨ì„± ê²€ì‚¬"""
        data = transaction.get('data', {})
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ['warehouse']
        for field in required_fields:
            if field not in data or pd.isna(data[field]):
                return False
                
        # ìˆ˜ëŸ‰ ë°ì´í„° í™•ì¸
        quantity_fields = ['incoming', 'outgoing', 'inventory']
        has_quantity = any(field in data and pd.notna(data[field]) and data[field] != 0 
                          for field in quantity_fields)
        
        return has_quantity
    
    def get_summary_statistics(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë”© ìš”ì•½ í†µê³„"""
        return {
            'total_files': len(self.raw_data),
            'total_transactions': len(self.processed_data),
            'warehouses': list(set(t['data'].get('warehouse') for t in self.processed_data if 'warehouse' in t['data'])),
            'date_range': self._get_date_range(),
            'file_summary': {filename: len(sheets) for filename, sheets in self.raw_data.items()}
        }
    
    def _get_date_range(self) -> Dict[str, Any]:
        """ë‚ ì§œ ë²”ìœ„ ê³„ì‚°"""
        dates = []
        for transaction in self.processed_data:
            if 'date' in transaction['data']:
                dates.append(transaction['data']['date'])
                
        if dates:
            return {
                'start_date': min(dates),
                'end_date': max(dates),
                'total_days': (max(dates) - min(dates)).days
            }
        return {} 